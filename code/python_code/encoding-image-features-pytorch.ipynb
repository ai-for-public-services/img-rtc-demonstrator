{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968046a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code for running the Autoencoder\n",
    "# Load in Dependencies\n",
    "\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from osgeo import gdal\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "\n",
    "from torchinfo import summary\n",
    "import gc\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm # progress bar\n",
    "from timeit import default_timer as timer\n",
    "def print_train_time(start:float,\n",
    "                    end:float,\n",
    "                    device: torch.device= None):\n",
    "    total_time=end-start\n",
    "    print(f\"Train time on {device} : {total_time:.3f} seconds\")\n",
    "    return total_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251b68b",
   "metadata": {},
   "source": [
    "# Load in  Filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38992ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the paths to all of the images in the 25m buffer folder\n",
    "# Images were created via the R code; raw data from https://digimap.edina.ac.uk/aerial\n",
    "\n",
    "inputPath1=\"../data/processed/cam-2016-25m\" # 16011 files\n",
    "inputPath2=\"../data/processed/cam-2020-25m\" # 16011 files\n",
    "inputPath3=\"../data/processed/glo-2018-25m\" # 16011 files\n",
    "inputPath4=\"../data/processed/glo-2021-25m\" # 16011 files\n",
    "inputPath5=\"../data/processed/oxf-2016-25m\" # 16011 files\n",
    "inputPath6=\"../data/processed/oxf-2019-25m\" # 16011 files\n",
    "folderpaths = [inputPath1,inputPath2,inputPath3,inputPath4,inputPath5,inputPath6]\n",
    "filelist = []\n",
    "\n",
    "# Load the images, and append them to a list.\n",
    "for folder in folderpaths: \n",
    "    for filepath in os.listdir(folder):\n",
    "        if filepath.endswith((\".tif\")):\n",
    "            #print(filepath)\n",
    "            tempfile=folder+'/{0}'.format(filepath)\n",
    "            filelist.append(tempfile)\n",
    "\n",
    "            len(filelist) # 107305 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the paths to all of the images in the 100m buffer folder\n",
    "# Images were created via the R code; raw data from https://digimap.edina.ac.uk/aerial\n",
    "\n",
    "inputPath1=\"../data/processed/cam-2016-100m\" \n",
    "inputPath2=\"../data/processed/cam-2020-100m\" \n",
    "inputPath3=\"../data/processed/glo-2018-100m\" \n",
    "inputPath4=\"../data/processed/glo-2021-100m\" \n",
    "inputPath5=\"../data/processed/oxf-2016-100m\" \n",
    "inputPath6=\"../data/processed/oxf-2019-100m\" \n",
    "folderpaths = [inputPath1,inputPath2,inputPath3,inputPath4,inputPath5,inputPath6]\n",
    "filelist_100m = []\n",
    "\n",
    "# Load the images, and append them to a list.\n",
    "for folder in folderpaths: \n",
    "    for filepath in os.listdir(folder):\n",
    "        if filepath.endswith((\".tif\")):\n",
    "            #print(filepath)\n",
    "            tempfile=folder+'/{0}'.format(filepath)\n",
    "            filelist_100m.append(tempfile)\n",
    "            \n",
    "len(filelist_100m) # 105441 files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2040868f",
   "metadata": {},
   "source": [
    "# Run a CAE - 100m example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloaders\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Create a DataLOader for my data\n",
    "class rgb25mdataset_train(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,filelist_100m, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.filelist = filelist_100m           \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()    \n",
    "        # Generate data\n",
    "        dataset = gdal.Open(self.filelist[idx])\n",
    "        image = dataset.ReadAsArray()  # Returned image is a NumPy array with shape (16, 60, 60) for example.\n",
    "        image = image/255\n",
    "\n",
    "        return image\n",
    "    \n",
    "class rgb25mdataset_test(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,filelist_100m, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.filelist = filelist_100m           \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()    \n",
    "        # Generate data\n",
    "        dataset = gdal.Open(self.filelist[idx])\n",
    "        image = dataset.ReadAsArray()  # Returned image is a NumPy array with shape (16, 60, 60) for example.\n",
    "        image = image/255\n",
    "\n",
    "        return image\n",
    "    \n",
    "my_dataset_train = rgb25mdataset_train(filelist_100m=filelist_100m)\n",
    "my_dataset_test = rgb25mdataset_test(filelist_100m=filelist_100m)\n",
    "\n",
    "my_dataloader_train = DataLoader(my_dataset_train, batch_size=20,shuffle=True, num_workers=0)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=20,shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "class Autoencoder_100m(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder_100m,self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3,padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.Conv2d(32, 64, kernel_size=3,padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            \n",
    "            nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            \n",
    "            nn.Conv2d(128,64,kernel_size=3,padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.Conv2d(64,64,kernel_size=3,padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64,32,kernel_size=3,padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "#             nn.Conv2d(32,32,kernel_size=3,padding=1),\n",
    "#             nn.LeakyReLU(True),\n",
    "            )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),# align_corners=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'), #align_corners=True),\n",
    "\n",
    "            #nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(64,64,kernel_size=3,stride=1,padding=0),#,output_padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(64,128,kernel_size=3,stride=1,padding=0),#,output_padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),# align_corners=True),\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(128,64,kernel_size=3,stride=1,padding=0),#,output_padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(64,64,kernel_size=3,stride=1,padding=0),#,output_padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),# align_corners=True),\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(64,32,kernel_size=3,stride=1,padding=0),#,output_padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(32,3,kernel_size=3,stride=1,padding=0),#,output_padding=1),\n",
    "            nn.Sigmoid())\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x    \n",
    "torch.manual_seed(42)\n",
    "cae_100m = Autoencoder_100m().to(device)\n",
    "cae_100m, summary(cae_100m,input_size= (1,3,200,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a7b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save best model\n",
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, model\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            torch.save(model.state_dict(), '../data/models/all_100m_best_model.pt')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4353dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can run a Train/Test Loop now\n",
    "# Loss Function\n",
    "loss_fn = nn.MSELoss() # sigmoid activation function built in\n",
    "\n",
    "# Opitimiser\n",
    "optimizer = optim.Adam(params=cae_100m.parameters(),\n",
    "                     lr=.001,\n",
    "                      weight_decay=1e-5)\n",
    "\n",
    "# calculate mae (how clsoe are pixels to correct?)\n",
    "def mae_fn(y_true,y_pred):\n",
    "    mae = torch.abs((y_true-y_pred)).sum()\n",
    "    return mae\n",
    "\n",
    "# Build the training Loop (and a testing loop)\n",
    "torch.manual_seed(42)\n",
    "epochs = 50\n",
    "\n",
    "# Instatiate datasets/loaders\n",
    "my_dataset_train = rgb25mdataset_train(filelist_100m=filelist_100m)\n",
    "my_dataset_test = rgb25mdataset_test(filelist_100m=filelist_100m)\n",
    "\n",
    "my_dataloader_train = DataLoader(my_dataset_train, batch_size=20,shuffle=True, num_workers=0)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=20,shuffle=False, num_workers=0)\n",
    "\n",
    "batch_size= 20\n",
    "\n",
    "train_time_start_on_cpu = timer()\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} out of {epochs}\")\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop through training batch data\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_train):\n",
    "        X = sample_batched.to(device)\n",
    "        X = Variable(X.float().cuda())\n",
    "        cae_100m.train()\n",
    "        \n",
    "        # Forward Pass\n",
    "        y_pred = cae_1(X)\n",
    "        \n",
    "        # Calc loss (per batch)\n",
    "        loss = loss_fn(y_pred, X)  # loss tested against itself\n",
    "        train_loss += loss # accumulate train loss\n",
    "        \n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # perform backpropagation on the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # performm gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        # track progress\n",
    "        if i_batch % 200 == 0:\n",
    "            print(f\"Batch {i_batch+1} out of {len(my_dataloader_train)} completed.\")\n",
    "            \n",
    "    # Divide total train loss by length of dataloader\n",
    "    train_loss /= (len(my_dataset_train)/batch_size)\n",
    "        \n",
    "    ### Testing\n",
    "    test_loss, test_mae = 0,0\n",
    "    \n",
    "    cae_100m.eval()\n",
    "    with torch.inference_mode():\n",
    "         for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "            X = sample_batched.to(device)\n",
    "            X = Variable(X.float().cuda())\n",
    "            \n",
    "            # Forward pass\n",
    "            test_pred = cae_100m(X)\n",
    "            \n",
    "            # loss accumulate\n",
    "            test_loss += loss_fn(test_pred,X)\n",
    "            \n",
    "            #Accuracy accumulate\n",
    "            test_mae += mae_fn(y_true=X, y_pred=test_pred)\n",
    "         \n",
    "         # get loss per batch\n",
    "         test_loss /= (len(my_dataset_test)/batch_size)\n",
    "         \n",
    "         # get mae per batch\n",
    "         test_mae /= (len(my_dataset_test)/batch_size)\n",
    "        \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_mae:.2f}\")\n",
    "    save_best_model(test_loss, epoch, cae_100m)\n",
    "\n",
    "\n",
    "train_time_end_on_cpu = timer()    \n",
    "total_train_time_on_cpu= print_train_time(start=train_time_start_on_cpu,\n",
    "                                          end=train_time_end_on_cpu,\n",
    "                                          device=str(next(cae_1.parameters()).device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077a63d",
   "metadata": {},
   "source": [
    "# Extract Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ed9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cae_100m.state_dict()\n",
    "\n",
    "cae_100m.load_state_dict(torch.load(\"../data/models/all_100m_best_model.pt\"))\n",
    "\n",
    "\n",
    "my_dataset_train = rgb25mdataset_train(filelist_train=filelist_100m)\n",
    "my_dataset_test = rgb25mdataset_test(filelist_test=filelist_100m)\n",
    "my_dataloader_train = DataLoader(my_dataset_train, batch_size=25,shuffle=True, num_workers=0)\n",
    "my_dataloader_test = DataLoader(my_dataset_test, batch_size=25,shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# using just the encoding layers, get the image features... for now just of the test set..\n",
    "prediction_list=[]\n",
    "cae_1.eval()\n",
    "with torch.inference_mode():\n",
    "    for i_batch, sample_batched in enumerate(my_dataloader_test):\n",
    "        X = sample_batched.to(device)\n",
    "        X = Variable(X.float().cuda())\n",
    "        test_pred = cae_1.encoder(X)\n",
    "        prediction_list.append(test_pred.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the items in the list and then flatten to get one row of features per image\n",
    "arr = np.vstack(prediction_list)\n",
    "arr = arr.reshape(-1,4608)\n",
    "\n",
    "# convert to df\n",
    "df = pd.DataFrame(arr)\n",
    "df[\"file_path\"] = filelist_100m\n",
    "\n",
    "# save a df of the image features\n",
    "df.to_csv(\"../data/processed/image-features/cae-100m-features.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
